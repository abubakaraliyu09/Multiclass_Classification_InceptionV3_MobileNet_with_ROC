{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311db7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "#from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, partial, rand, space_eval\n",
    "# from sklearn.metrics import log_loss\n",
    "import sys\n",
    "# import time\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "# from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "# from glob import glob\n",
    "# import cv2\n",
    "# import skimage\n",
    "# from skimage.transform import resize\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# from keras import models\n",
    "# from keras import optimizers\n",
    "# from keras.models import load_model\n",
    "# import keras.callbacks as kcall\n",
    "from keras.optimizers import Adam, RMSprop,SGD\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Input,concatenate\n",
    "# from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.applications import DenseNet201\n",
    "from keras.applications import DenseNet121\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications import NASNetLarge, NASNetMobile\n",
    "from keras.applications import MobileNet\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "# from keras.applications.vgg19 import VGG19\n",
    "from keras.regularizers import l2, l1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from keras.layers import Input, concatenate\n",
    "# from keras import optimizers, metrics, models\n",
    "# from keras.layers import Input, Flatten, Dense\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"tensorflow Version\", tf.__version__)\n",
    "# print(\"dim_ordering:\", K.image_ordering_format())\n",
    "\n",
    "batch_size = 2\n",
    "img_height, img_width = 256, 256\n",
    "input_shape = (img_height, img_width, 3)\n",
    "epochs = 50\n",
    "\n",
    "#DATA PATHS\n",
    "train_dir = 'C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\Train\\\\'\n",
    "test_dir = 'C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\Test\\\\'\n",
    "\n",
    "#PREPROCESSING\n",
    "def preprocess_input(x):\n",
    "    # 'RGB'->'BGR'\n",
    "    x = x[:, :, ::-1]\n",
    "    # Zero-center by imagenet mean pixel\n",
    "    x[:, :, 0] -= 103.939\n",
    "    x[:, :, 1] -= 116.779\n",
    "    x[:, :, 2] -= 123.68\n",
    "    return x\n",
    "\n",
    "random_seed = np.random.seed(1142)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "    preprocessing_function = preprocess_input,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='reflect',\n",
    "    validation_split= 0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    subset = 'training',\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    subset = 'validation',\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255, preprocessing_function = preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\n",
    "predict_size_test = int(math.ceil(nb_test_samples / batch_size))\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "print(\"nb_train_samples:\", nb_train_samples)\n",
    "print(\"nb_validation_samples:\", nb_validation_samples)\n",
    "print(\"nb_test_samples:\", nb_test_samples)\n",
    "\n",
    "print(\"\\npredict_size_train:\", predict_size_train)\n",
    "print(\"predict_size_validation:\", predict_size_validation)\n",
    "print(\"predict_size_test:\", predict_size_test)\n",
    "\n",
    "print(\"\\n num_classes:\", num_classes)\n",
    "\n",
    "\n",
    "#PRETRAINED WEIGHTS\n",
    "vgg19_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "inception_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "vgg16_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "denseNet201_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "denseNet121_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "resenet50_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "inception_resnet_v2_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "nasnet_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\nasnet_large_no_top.h5\"\n",
    "nasnet_mobile_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\nasnet_mobile_no_top.h5\"\n",
    "mobilenet_weights =\"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\pretrained\\\\mobilenet_1_0_224_tf_no_top.h5\"\n",
    "\n",
    "#MODELS\n",
    "input_tensor = Input(shape = input_shape)  \n",
    "\n",
    "base_model1=InceptionV3(input_shape= input_shape,weights=inception_weights, include_top=False, input_tensor=input_tensor)\n",
    "base_model2=MobileNet(input_shape= input_shape,weights=mobilenet_weights, include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "x1 = base_model1.output\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "\n",
    "x2 = base_model2.output\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "\n",
    "merge = concatenate([x1, x2])\n",
    "predictions = Dense(num_classes, activation='softmax')(merge)\n",
    "\n",
    "model = Model(inputs=input_tensor,outputs=predictions)\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)\n",
    "    \n",
    "\n",
    "c1 = model.layers[11].output \n",
    "c1 = GlobalAveragePooling2D()(c1)       \n",
    "\n",
    "c2 = model.layers[18].output\n",
    "c2 = GlobalAveragePooling2D()(c2)       \n",
    "\n",
    "c3 = model.layers[28].output\n",
    "c3 = GlobalAveragePooling2D()(c3)       \n",
    "\n",
    "c4 = model.layers[51].output\n",
    "c4 = GlobalAveragePooling2D()(c4) \n",
    "\n",
    "c5 = model.layers[74].output\n",
    "c5 = GlobalAveragePooling2D()(c5) \n",
    "\n",
    "c6 = model.layers[101].output\n",
    "c6 = GlobalAveragePooling2D()(c6) \n",
    "\n",
    "c7 = model.layers[120].output\n",
    "c7 = GlobalAveragePooling2D()(c7) \n",
    "\n",
    "c8 = model.layers[167].output\n",
    "c8 = GlobalAveragePooling2D()(c8) \n",
    "\n",
    "c9 = model.layers[215].output\n",
    "c9 = GlobalAveragePooling2D()(c9) \n",
    "\n",
    "c10 = model.layers[263].output\n",
    "c10 = GlobalAveragePooling2D()(c10) \n",
    "\n",
    "c11 = model.layers[313].output\n",
    "c11 = GlobalAveragePooling2D()(c11) \n",
    "\n",
    "c12 = model.layers[334].output\n",
    "c12 = GlobalAveragePooling2D()(c12) \n",
    "\n",
    "c13 = model.layers[376].output\n",
    "c13 = GlobalAveragePooling2D()(c13) \n",
    "\n",
    "c14 = model.layers[291].output\n",
    "c14 = GlobalAveragePooling2D()(c14) \n",
    "\n",
    "c15 = model.layers[305].output\n",
    "c15 = GlobalAveragePooling2D()(c15) \n",
    "\n",
    "c16 = model.layers[311].output\n",
    "c16 = GlobalAveragePooling2D()(c16) \n",
    "\n",
    "con = concatenate([c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16])\n",
    "\n",
    "bottleneck_final_model = Model(inputs=model.input, outputs=con)\n",
    "\n",
    "\n",
    "os.mkdir( \"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\extracted_features\")\n",
    "extracted_features_dir = \"C:\\\\Users\\\\aleey\\\\Desktop\\\\White Blood Cells\\\\Attention Model\\\\extracted_features\\\\\"\n",
    "model_name = \"densenet201_InceptionResNetV2_descriptors\"\n",
    "\n",
    "#EXTRACT FEATURES AND SAVE TO DIR\n",
    "bottleneck_features_train = bottleneck_final_model.predict_generator(train_generator, predict_size_train)\n",
    "np.save(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy', bottleneck_features_train)\n",
    "\n",
    "bottleneck_features_validation = bottleneck_final_model.predict_generator(validation_generator, predict_size_validation)\n",
    "np.save(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy', bottleneck_features_validation)\n",
    "\n",
    "bottleneck_features_test = bottleneck_final_model.predict_generator(test_generator, predict_size_test)\n",
    "np.save(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy', bottleneck_features_test)\n",
    "\n",
    "#LOAD THE EXTRACTED FEATURES\n",
    "train_data = np.load(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy')\n",
    "validation_data = np.load(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy')\n",
    "test_data = np.load(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy')\n",
    "\n",
    "train_labels = train_generator.classes\n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "\n",
    "validation_labels = validation_generator.classes\n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "test_labels = test_generator.classes\n",
    "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
    "\n",
    "#DEFINE THE CLASSIFIER & TRAIN THE MODEL\n",
    "model = Sequential()\n",
    "model.add(Dense(4096, activation='tanh',kernel_regularizer=l2(1e-05),  bias_regularizer=l2(1e-06), activity_regularizer=l1(0.001)))\n",
    "model.add(Dense(128, activation='tanh',kernel_regularizer=l2(1e-05),  bias_regularizer=l2(1e-06), activity_regularizer=l1(0.001)))\n",
    "model.add(Dense(64, activation='tanh',kernel_regularizer=l2(1e-05),  bias_regularizer=l2(1e-06), activity_regularizer=l1(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "adam_opt=Adam(lr = 0.0001, beta_1=0.6, beta_2=0.8)\n",
    "model.compile(optimizer=adam_opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_data, train_labels,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(validation_data, validation_labels),\n",
    "                    verbose= 1)\n",
    "\n",
    "#PLOT THE RESULTS\n",
    "import seaborn\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Acc', 'Test Acc'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], 'o-')\n",
    "plt.plot(history.history['val_loss'], 'x-')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Test Loss'], loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(8,4))\n",
    "N = epochs\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"center right\")\n",
    "\n",
    "\n",
    "#EVALUATE THE MODEL\n",
    "(eval_loss, eval_accuracy) = model.evaluate(validation_data, validation_labels, batch_size= batch_size, verbose=1)\n",
    "\n",
    "print(\"Validation Accuracy: {:.4f}%\".format(eval_accuracy * 100))\n",
    "print(\"Validation Loss: {}\".format(eval_loss))\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score\n",
    "\n",
    "preds = model.predict(test_data)\n",
    "\n",
    "predictions = [i.argmax() for i in preds]\n",
    "y_true = [i.argmax() for i in test_labels]\n",
    "cm = confusion_matrix(y_pred=predictions, y_true=y_true)\n",
    "\n",
    "print('Accuracy {}'.format(accuracy_score(y_true=y_true, y_pred=predictions)))\n",
    "\n",
    "#PLOT THE CONFUSION MATRIX\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "labels = []\n",
    "\n",
    "label = test_generator.class_indices\n",
    "indexlabel = dict((value, key) for key, value in label.items())\n",
    "\n",
    "for k,v in indexlabel.items():\n",
    "    labels.append(v)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    print(cm)\n",
    "#     fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "#     plt.savefig('plots/3.InceptionV3-2-Private-DataSet-CM.png', bbox_inches='tight', dpi = 100) \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, classes=labels, title=' ')\n",
    "\n",
    "#PERFORMACE EVALUATION\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred=predictions\n",
    "y_pred_probabilities=y_pred\n",
    "\n",
    "# y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = y_true\n",
    "\n",
    "classnames=[]\n",
    "for classname in test_generator.class_indices:\n",
    "    classnames.append(classname)\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_actual, y_pred) \n",
    "print(confusion_mtx)\n",
    "target_names = classnames\n",
    "print(classification_report(y_actual, y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "y_pred_class = model.predict(test_data, verbose=1)\n",
    "\n",
    "y_pred_class = [np.argmax(r) for r in y_pred_class]\n",
    "test_y = [np.argmax(r) for r in test_labels]\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Precision\n",
    "print('Precision = ', precision_score(test_y, y_pred_class, average='weighted'))\n",
    "# (None, 'micro', 'macro', 'weighted', 'samples')\n",
    "\n",
    "# Recall\n",
    "print('Recall = ', recall_score(test_y, y_pred_class, average='weighted'))\n",
    "\n",
    "# f1_score\n",
    "print('f1_score = ', f1_score(test_y, y_pred_class, average='weighted'))\n",
    "\n",
    "\n",
    "#PLOT THE ROC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"weighted\"):\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(y_test)\n",
    "\n",
    "    truth = label_binarizer.transform(y_test)\n",
    "    pred = label_binarizer.transform(y_pred)\n",
    "    return roc_auc_score(truth, pred, average=average)\n",
    "# roc_auc_score\n",
    "print('roc_auc_score = ', multiclass_roc_auc_score(test_y, y_pred_class))\n",
    "\n",
    "#ROC Curve for each class\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "predict_class = np.argmax(preds, axis=1)\n",
    "\n",
    "y_pred = preds\n",
    "y_pred_probabilities=y_pred\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = test_generator.classes\n",
    "\n",
    "classnames=[]\n",
    "for classname in test_generator.class_indices:\n",
    "    classnames.append(classname)\n",
    "\n",
    "y_actual_binary = label_binarize(y_actual, classes=[0, 1, 2, 3, 4])\n",
    "y_pred_binary = y_pred_probabilities\n",
    "n_classes=5\n",
    "lw=2\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_actual_binary[:, i], y_pred_binary[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_actual_binary.ravel(), y_pred_binary.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "colors = cycle(['red','blue','green','yellow','orange', 'aqua', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(classnames[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-class')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
